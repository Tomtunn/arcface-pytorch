{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter, DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_df(identity_root, identity_list):\n",
    "    path_list = []\n",
    "    identity_label_list = []\n",
    "    for identity in tqdm(identity_list):\n",
    "        image_list = os.listdir(os.path.join(identity_root, identity)) \n",
    "        for img in image_list:\n",
    "            path = os.path.join(identity_root, identity, img)\n",
    "            path_list.append(path)\n",
    "            identity_label_list.append(identity) \n",
    "\n",
    "    data_df = pd.DataFrame({'img_path': path_list, 'identity': identity_label_list})\n",
    "    data_df['identity_code'] = pd.Categorical(data_df['identity']).codes # convert identity to unique code(int)\n",
    "    data_df['identity_code'] = data_df['identity_code'].astype('int32')\n",
    "    return data_df\n",
    "\n",
    "# save numpy array from path\n",
    "def get_img_npy(path_df, img_shape):\n",
    "    img_npy = np.zeros((len(path_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    label_npy = np.zeros((len(path_df), 1), dtype=np.uint8)\n",
    "    for i in tqdm(range(len(path_df)), desc='get_img_npy'):\n",
    "        img_path = path_df.iloc[i]['img_path']\n",
    "        img = Image.open(img_path)\n",
    "        img_npy[i] = np.array(img)\n",
    "        label_npy[i] = path_df.iloc[i]['identity_code']\n",
    "    return img_npy, label_npy \n",
    "\n",
    "def save_chunk_npy(path_df, num_chunk, img_shape, root):\n",
    "    path_df_list = np.array_split(path_df, num_chunk)\n",
    "    for i in range(len(path_df_list)):\n",
    "        img_npy, label_npy = get_img_npy(path_df_list[i], img_shape)\n",
    "        np.save(os.path.join(root, 'data_{}.npy'.format(i)), img_npy)\n",
    "        np.save(os.path.join(root, 'label_{}.npy'.format(i)), label_npy)\n",
    "\n",
    "def get_pair_npy(pair_df, img_shape):\n",
    "    img_npy1 = np.zeros((len(pair_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    img_npy2 = np.zeros((len(pair_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    label_npy = np.zeros((len(pair_df), 1), dtype=np.uint8)\n",
    "    for i in tqdm(range(len(pair_df)), desc='get_pair_npy'):\n",
    "        img_path1 = pair_df.iloc[i]['img1_path']\n",
    "        img_path2 = pair_df.iloc[i]['img2_path']\n",
    "        img1 = Image.open(img_path1)\n",
    "        img2 = Image.open(img_path2)\n",
    "        img_npy1[i] = np.array(img1)\n",
    "        img_npy2[i] = np.array(img2)\n",
    "        label_npy[i] = pair_df.iloc[i]['label']\n",
    "    return img_npy1, img_npy2, label_npy\n",
    "\n",
    "def save_chunk_pair_npy(pair_df, num_chunk, img_shape, root):\n",
    "    pair_df_list = np.array_split(pair_df, num_chunk)\n",
    "    for i in range(num_chunk):\n",
    "        img_npy1, img_npy2, label_npy = get_pair_npy(pair_df_list[i], img_shape)\n",
    "        np.save(os.path.join(root, 'img1_{}.npy'.format(i)), img_npy1)\n",
    "        np.save(os.path.join(root, 'img2_{}.npy'.format(i)), img_npy2)\n",
    "        np.save(os.path.join(root, 'label_{}.npy'.format(i)), label_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # buid the dataframe from the the path\n",
    "# identity_root = 'lfw_funneled'\n",
    "# identity_list = os.listdir(identity_root)\n",
    "# identity_list = [identity for identity in identity_list if os.path.isdir(os.path.join(identity_root, identity))] #only folder is identity\n",
    "\n",
    "# # total data for test\n",
    "# path_df = get_path_df(identity_root, identity_list)\n",
    "\n",
    "# # train test val split\n",
    "# train_df, test_df = train_test_split(path_df, test_size=0.2, random_state=42)\n",
    "# test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "# print('train: {}, val: {}, test: {}'.format(len(train_df), len(val_df), len(test_df)))\n",
    "\n",
    "# # split the path_df into 10 chunks\n",
    "# num_chunk = 1\n",
    "# npy_root = 'lfw_funneled_npy'\n",
    "# img_shape = (250, 250, 3)\n",
    "# save_chunk_npy(train_df, num_chunk, img_shape, npy_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, img_npy_list, label_npy_list, input_shape, phase=\"train\"):\n",
    "        self.img_npy = np.vstack(img_npy_list)\n",
    "        self.label_npy = np.vstack(label_npy_list)\n",
    "        self.phase = phase\n",
    "        self.input_shape = input_shape\n",
    "        if self.phase == 'train':\n",
    "            self.transforms = T.Compose([\n",
    "                T.RandomCrop(self.input_shape[1:]),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "        else:   \n",
    "            self.transforms = T.Compose([\n",
    "                T.CenterCrop(self.input_shape[1:]),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.label_npy.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.img_npy[index]\n",
    "        label = self.label_npy[index]\n",
    "        data = Image.fromarray(data)\n",
    "        data = data.convert('RGB')\n",
    "        data = self.transforms(data)\n",
    "        return data.float(), label.squeeze()\n",
    "    \n",
    "class PairFaceDataset(Dataset):\n",
    "    def __init__(self, img_npy_list1, img_npy_list2, label_npy_list, input_shape):\n",
    "        self.img_npy1 = np.vstack(img_npy_list1)\n",
    "        self.img_npy2 = np.vstack(img_npy_list2)\n",
    "        self.label_npy = np.vstack(label_npy_list)\n",
    "        self.input_shape = input_shape\n",
    "        self.transforms = T.Compose([\n",
    "            T.CenterCrop(self.input_shape[1:]),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.label_npy.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data1 = self.img_npy1[index]\n",
    "        data1 = Image.fromarray(data1)\n",
    "        data1 = data1.convert('RGB')\n",
    "        data1 = self.transforms(data1)\n",
    "\n",
    "        data2 = self.img_npy2[index]\n",
    "        data2 = Image.fromarray(data2)\n",
    "        data2 = data2.convert('RGB')\n",
    "        data2 = self.transforms(data2)\n",
    "        label = self.label_npy[index]\n",
    "        return data1.float(), data2.float(), label.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c506d3433641368c5d1ba40df89739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10585, val: 1324, test: 1323\n",
      "5749\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "npy_root = 'lfw_funneled_npy'\n",
    "identity_root = 'lfw_funneled'\n",
    "identity_list = os.listdir(identity_root)\n",
    "identity_list = [identity for identity in identity_list if os.path.isdir(os.path.join(identity_root, identity))] #only folder is identity\n",
    "\n",
    "# total data for test\n",
    "path_df = get_path_df(identity_root, identity_list)\n",
    "\n",
    "# train test val split\n",
    "train_df, test_df = train_test_split(path_df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "print('train: {}, val: {}, test: {}'.format(len(train_df), len(val_df), len(test_df)))\n",
    "\n",
    "img_npy_list = [np.load(os.path.join(npy_root, 'data_1.npy'))]\n",
    "label_npy_list = [np.load(os.path.join(npy_root, 'label_1.npy'))]\n",
    "\n",
    "input_shape = (1, 250, 250)\n",
    "phase = 'train'\n",
    "dataset = FaceDataset(img_npy_list, label_npy_list, input_shape, phase)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "num_classes = len(np.unique(path_df['identity'].values))\n",
    "print(num_classes)\n",
    "\n",
    "# pair dataloader\n",
    "root = 'data/test_pair'\n",
    "img_npy_list1 = [np.load(os.path.join(root, 'img1_0.npy'))]\n",
    "img_npy_list2 = [np.load(os.path.join(root, 'img2_0.npy'))]\n",
    "label_npy_list = [np.load(os.path.join(root, 'label_0.npy'))]\n",
    "\n",
    "input_shape = (3, 250, 250)\n",
    "pair_dataset = PairFaceDataset(img_npy_list1, img_npy_list2, label_npy_list, input_shape)\n",
    "pair_dataloader = DataLoader(pair_dataset, batch_size=16, shuffle=True)\n",
    "num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\earth\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\earth\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_resnet = resnet18(pretrained=False)\n",
    "# change first and last layer\n",
    "model_resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_resnet.fc = nn.Linear(512, 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(model, img1, img2):\n",
    "    with torch.no_grad():\n",
    "        img1 = img1.cuda()\n",
    "        img2 = img2.cuda()\n",
    "        feature1 = model(img1)\n",
    "        feature2 = model(img2)\n",
    "    return feature1, feature2\n",
    "\n",
    "def cosin_metric(feature1, feature2):\n",
    "    return F.cosine_similarity(feature1, feature2)\n",
    "\n",
    "def get_acc(y_score, y_true):\n",
    "    thresholds = sorted(set(y_score), reverse=True)\n",
    "    best_acc = 0\n",
    "    best_th = 0\n",
    "    for th in thresholds:\n",
    "        y_pred = (y_score >= th).astype(int)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_th = th\n",
    "    return best_acc, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "data_df = val_df.copy()\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# dataloader\n",
    "train_dataset = FaceDataset(img_npy_list, label_npy_list, input_shape, 'train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "num_identity = len(np.unique(train_df['identity']))\n",
    "validate_every =(len(train_dataset) // batch_size) // 2 # validate every half epoch\n",
    "\n",
    "# validate dataloader\n",
    "# val_batch_size = 16\n",
    "# root = 'data/test_pair'\n",
    "# img_npy_list1 = [np.load(os.path.join(root, 'img1_0.npy'))]\n",
    "# img_npy_list2 = [np.load(os.path.join(root, 'img2_0.npy'))]\n",
    "# label_npy_list = [np.load(os.path.join(root, 'label_0.npy'))]\n",
    "\n",
    "# input_shape = (3, 250, 250)\n",
    "# pair_dataset = PairFaceDataset(img_npy_list1, img_npy_list2, label_npy_list, input_shape)\n",
    "# pair_dataloader = DataLoader(pair_dataset, batch_size=val_batch_size, shuffle=True)\n",
    "# num_classes = 2\n",
    "\n",
    "# model, metric_fc, criterion, optimizer\n",
    "model = DataParallel(model_resnet)\n",
    "metric_fc = DataParallel(ArcMarginProduct(512, num_classes, s=30, m=0.5, easy_margin=False))\n",
    "model.to(device)\n",
    "metric_fc.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "weigth_decay = 5e-4\n",
    "optimizer = torch.optim.SGD([{'params': model.parameters()}, {'params': metric_fc.parameters()}], \n",
    "                                    lr=learning_rate, weight_decay=weigth_decay)\n",
    "learning_step = 1\n",
    "scheduler = StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "\n",
    "# writer\n",
    "exist_path = os.listdir('log')\n",
    "writer_version = int(exist_path[-1][-1]) + 1 if len(exist_path) > 0 else 0\n",
    "writer = SummaryWriter(log_dir=\"log/v\" + str(writer_version))\n",
    "step = 0\n",
    "epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bc31099628443d8c8c25ee0a4cde50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f4d47434ce411bbe5930bef073d44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca27fc42b1d4f2a8f4cc429f7e8d4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859426da5dbd43198bb862c7b31de2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9b5f9241db4084b0549911aeaeef30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923778f82b0b405182f48ad39604791f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e0c1b0667d47b2a8a27b921aaee267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee9bf1f60524f1db045819c10ca67f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccd9fca4e6b4de084da7027efa5f41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c679014c0b481590f1df86fc73c122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c5b538ac8e43c1aca340a55b0eebf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62758a07287449cc9a9f7e27f064d12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea87537473384607805e4def4ce7467d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388e05f7c11742d7b97a0dee9b7258fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29076ce7b7584f61b413606398491bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ad20d6c28b4f50ba2bd98cc3f2ec63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48a40d9ea024e299638c518a1f6eae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e5a7ec2bd640c396d40286ec8dc6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e4c572f41e470e9210b7b9ae802b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059bb7ead25249708648d8b16a86119a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/20:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for data, train_label in tqdm(train_dataloader, desc='Epoch {}/{}'.format(epoch+1, num_epochs)):\n",
    "        # forward\n",
    "        data = data.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        feature = model(data).to(device)\n",
    "        output = metric_fc(feature, train_label).to(device)\n",
    "        loss = criterion(output, train_label.long().cuda())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validate\n",
    "        if step % validate_every == 0:\n",
    "            model.eval()\n",
    "            similarity_list = []\n",
    "            label_list = []\n",
    "            with torch.no_grad():\n",
    "                for data1, data2, label in pair_dataloader:\n",
    "                    data1 = data1.to(device)\n",
    "                    data2 = data2.to(device)\n",
    "                    label = label.to(device)\n",
    "                    feature1, feature2 = get_feature(model, data1, data2)\n",
    "                    similarity = cosin_metric(feature1, feature2)\n",
    "                    similarity_list.append(similarity.cpu().numpy())\n",
    "                    label_list.append(label.cpu().numpy())\n",
    "                similarity_list = np.concatenate(similarity_list)\n",
    "                label_list = np.concatenate(label_list)\n",
    "                acc, th = get_acc(similarity_list, label_list)   \n",
    "            model.train()\n",
    "\n",
    "            # visualize\n",
    "            # img_grid = make_grid(data)\n",
    "            # img = data.cpu().numpy()\n",
    "            # img = img.reshape(img.shape[0], -1)\n",
    "            # output = output.data.cpu().numpy()\n",
    "            # output = np.argmax(output, axis=1)\n",
    "            # train_label = train_label.data.cpu().numpy()\n",
    "\n",
    "            # writer.add_histogram('fc', model_resnet18.fc.weight, epoch)\n",
    "            # writer.add_image('image', img_grid, step)\n",
    "            writer.add_scalar('loss', loss.item(), step)\n",
    "            writer.add_scalar('acc', acc, step)\n",
    "            writer.add_scalar('threshold', th, step)\n",
    "            writer.add_scalar('lr', scheduler.get_last_lr()[0], step)\n",
    "            # writer.add_embedding(img, metadata=train_label, label_img=data, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (207294078.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [14], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    'optimizer_state_dict': optimzer.state_dict()\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_path = 'model_save/model_resnet18.pth'\n",
    "metric_fc_path = 'model_save/metric_fc.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'losses': loss,\n",
    "    'acc': acc,\n",
    "    'step': step,\n",
    "    'epoch_count': epoch_count}, model_path)\n",
    "torch.save(metric_fc.state_dict(), metric_fc_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_path = 'model_save/model_resnet18.pth'\n",
    "metric_fc_path = 'model_save/metric_fc.pth'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
