{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter, DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet18\n",
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# lr scheduler\n",
    "# weight decay (optimizer)\n",
    "# weight decay (lr scheduler)\n",
    "\n",
    "# validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_df(identity_root, identity_list):\n",
    "    path_list = []\n",
    "    identity_label_list = []\n",
    "    for identity in tqdm(identity_list):\n",
    "        image_list = os.listdir(os.path.join(identity_root, identity)) \n",
    "        for img in image_list:\n",
    "            path = os.path.join(identity_root, identity, img)\n",
    "            path_list.append(path)\n",
    "            identity_label_list.append(identity) \n",
    "\n",
    "    data_df = pd.DataFrame({'img_path': path_list, 'identity': identity_label_list})\n",
    "    data_df['identity_code'] = pd.Categorical(data_df['identity']).codes # convert identity to unique code(int)\n",
    "    data_df['identity_code'] = data_df['identity_code'].astype('int32')\n",
    "    return data_df\n",
    "\n",
    "# save numpy array from path\n",
    "def get_img_npy(path_df, img_shape):\n",
    "    img_npy = np.zeros((len(path_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    label_npy = np.zeros((len(path_df), 1), dtype=np.uint8)\n",
    "    for i in tqdm(range(len(path_df)), desc='get_img_npy'):\n",
    "        img_path = path_df.iloc[i]['img_path']\n",
    "        img = Image.open(img_path)\n",
    "        img_npy[i] = np.array(img)\n",
    "        label_npy[i] = path_df.iloc[i]['identity_code']\n",
    "    return img_npy, label_npy \n",
    "\n",
    "def save_chunk_npy(path_df, num_chunk, img_shape, root):\n",
    "    path_df_list = np.array_split(path_df, num_chunk)\n",
    "    for i in range(len(path_df_list)):\n",
    "        img_npy, label_npy = get_img_npy(path_df_list[i], img_shape)\n",
    "        np.save(os.path.join(root, 'data_{}.npy'.format(i)), img_npy)\n",
    "        np.save(os.path.join(root, 'label_{}.npy'.format(i)), label_npy)\n",
    "\n",
    "def get_pair_npy(pair_df, img_shape):\n",
    "    img_npy1 = np.zeros((len(pair_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    img_npy2 = np.zeros((len(pair_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    label_npy = np.zeros((len(pair_df), 1), dtype=np.uint8)\n",
    "    for i in tqdm(range(len(pair_df)), desc='get_pair_npy'):\n",
    "        img_path1 = pair_df.iloc[i]['img1_path']\n",
    "        img_path2 = pair_df.iloc[i]['img2_path']\n",
    "        img1 = Image.open(img_path1)\n",
    "        img2 = Image.open(img_path2)\n",
    "        img_npy1[i] = np.array(img1)\n",
    "        img_npy2[i] = np.array(img2)\n",
    "        label_npy[i] = pair_df.iloc[i]['label']\n",
    "    return img_npy1, img_npy2, label_npy\n",
    "\n",
    "def save_chunk_pair_npy(pair_df, num_chunk, img_shape, root):\n",
    "    pair_df_list = np.array_split(pair_df, num_chunk)\n",
    "    for i in range(num_chunk):\n",
    "        img_npy1, img_npy2, label_npy = get_pair_npy(pair_df_list[i], img_shape)\n",
    "        np.save(os.path.join(root, 'img1_{}.npy'.format(i)), img_npy1)\n",
    "        np.save(os.path.join(root, 'img2_{}.npy'.format(i)), img_npy2)\n",
    "        np.save(os.path.join(root, 'label_{}.npy'.format(i)), label_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a653e53895140509bcd6d0dc602df3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 10585, val: 1324, test: 1323\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803f2083192f441fae3cb37b688f906c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "get_img_npy:   0%|          | 0/10585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# buid the dataframe from the the path\n",
    "identity_root = 'lfw_funneled'\n",
    "identity_list = os.listdir(identity_root)\n",
    "identity_list = [identity for identity in identity_list if os.path.isdir(os.path.join(identity_root, identity))] #only folder is identity\n",
    "\n",
    "# total data for test\n",
    "path_df = get_path_df(identity_root, identity_list)\n",
    "\n",
    "# train test val split\n",
    "train_df, test_df = train_test_split(path_df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "print('train: {}, val: {}, test: {}'.format(len(train_df), len(val_df), len(test_df)))\n",
    "\n",
    "# split the path_df into 10 chunks\n",
    "num_chunk = 1\n",
    "npy_root = 'lfw_funneled_npy'\n",
    "img_shape = (250, 250, 3)\n",
    "save_chunk_npy(train_df, num_chunk, img_shape, npy_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, img_npy_list, label_npy_list, input_shape, phase=\"train\"):\n",
    "        self.img_npy = np.vstack(img_npy_list)\n",
    "        self.label_npy = np.vstack(label_npy_list)\n",
    "        self.phase = phase\n",
    "        self.input_shape = input_shape\n",
    "        if self.phase == 'train':\n",
    "            self.transforms = T.Compose([\n",
    "                T.RandomCrop(self.input_shape[1:]),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "        else:   \n",
    "            self.transforms = T.Compose([\n",
    "                T.CenterCrop(self.input_shape[1:]),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.label_npy.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.img_npy[index]\n",
    "        label = self.label_npy[index]\n",
    "        data = Image.fromarray(data)\n",
    "        data = data.convert('RGB')\n",
    "        data = self.transforms(data)\n",
    "        return data.float(), label.squeeze()\n",
    "    \n",
    "class PairFaceDataset(Dataset):\n",
    "    def __init__(self, img_npy_list1, img_npy_list2, label_npy_list, input_shape):\n",
    "        self.img_npy1 = np.vstack(img_npy_list1)\n",
    "        self.img_npy2 = np.vstack(img_npy_list2)\n",
    "        self.label_npy = np.vstack(label_npy_list)\n",
    "        self.input_shape = input_shape\n",
    "        self.transforms = T.Compose([\n",
    "            T.CenterCrop(self.input_shape[1:]),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.label_npy.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data1 = self.img_npy1[index]\n",
    "        data1 = Image.fromarray(data1)\n",
    "        data1 = data1.convert('RGB')\n",
    "        data1 = self.transforms(data1)\n",
    "\n",
    "        data2 = self.img_npy2[index]\n",
    "        data2 = Image.fromarray(data2)\n",
    "        data2 = data2.convert('RGB')\n",
    "        data2 = self.transforms(data2)\n",
    "        label = self.label_npy[index]\n",
    "        return data1.float(), data2.float(), label.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "img_npy_list = [np.load(os.path.join(npy_root, 'data_1.npy'))]\n",
    "label_npy_list = [np.load(os.path.join(npy_root, 'label_1.npy'))]\n",
    "\n",
    "input_shape = (1, 250, 250)\n",
    "phase = 'train'\n",
    "dataset = FaceDataset(img_npy_list, label_npy_list, input_shape, phase)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "num_classes = len(np.unique(path_df['identity'].values))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare data for cosine similarity test\n",
    "# data_df = test_df.copy()\n",
    "\n",
    "# # find positive pairs\n",
    "# anchor_indexs = data_df.groupby('identity_code').filter(lambda x: len(x) > 1).index\n",
    "# anchor_df = data_df.loc[data_df.index.isin(anchor_indexs)]\n",
    "# anchors = np.unique(anchor_df['identity_code'].values)\n",
    "# anchor_path1s = []\n",
    "# anchor_path2s = []\n",
    "\n",
    "# for anchor in anchors:\n",
    "#     anchor_identities = anchor_df[anchor_df['identity_code'] == anchor]\n",
    "#     anchor_path1, anchor_path2 = np.random.choice(anchor_identities['img_path'].values, size=2, replace=False)\n",
    "#     anchor_path1s.append(anchor_path1)\n",
    "#     anchor_path2s.append(anchor_path2)\n",
    "# labels = [1] * len(anchor_path1s)\n",
    "\n",
    "\n",
    "# # find negative pairs\n",
    "# size = len(anchor_path1s)\n",
    "# anchors = np.unique(data_df['identity_code'].values)\n",
    "# for i in range(size):\n",
    "#     anchor1, anchor2 = np.random.choice(anchors, size=2, replace=False)\n",
    "#     anchor_identities1 = data_df[data_df['identity_code'] == anchor1]\n",
    "#     anchor_identities2 = data_df[data_df['identity_code'] == anchor2]\n",
    "#     anchor_path1 = np.random.choice(anchor_identities1['img_path'].values)\n",
    "#     anchor_path2 = np.random.choice(anchor_identities2['img_path'].values)\n",
    "#     anchor_path1s.append(anchor_path1)\n",
    "#     anchor_path2s.append(anchor_path2)\n",
    "\n",
    "# labels += [0] * size\n",
    "# pair_df = pd.DataFrame({'img1_path':anchor_path1s, 'img2_path':anchor_path2s, 'label':labels})\n",
    "# pair_df = pair_df.copy()\n",
    "# img_shape = (250, 250, 3)\n",
    "# num_chunks = 1\n",
    "# root = 'data/test_pair'\n",
    "# save_chunk_pair_npy(pair_df, num_chunks, img_shape, root)\n",
    "# pair_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair dataloader\n",
    "root = 'data/test_pair'\n",
    "img_npy_list1 = [np.load(os.path.join(root, 'img1_0.npy'))]\n",
    "img_npy_list2 = [np.load(os.path.join(root, 'img2_0.npy'))]\n",
    "label_npy_list = [np.load(os.path.join(root, 'label_0.npy'))]\n",
    "\n",
    "input_shape = (3, 250, 250)\n",
    "pair_dataset = PairFaceDataset(img_npy_list1, img_npy_list2, label_npy_list, input_shape)\n",
    "pair_dataloader = DataLoader(pair_dataset, batch_size=16, shuffle=True)\n",
    "num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\earth\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\earth\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load pretrained model\n",
    "model_resnet = resnet18(pretrained=False)\n",
    "# change first and last layer\n",
    "model_resnet.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_resnet.fc = nn.Linear(512, 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(model, img1, img2):\n",
    "    with torch.no_grad():\n",
    "        img1 = img1.cuda()\n",
    "        img2 = img2.cuda()\n",
    "        feature1 = model(img1)\n",
    "        feature2 = model(img2)\n",
    "    return feature1, feature2\n",
    "\n",
    "def cosin_metric(feature1, feature2):\n",
    "    return F.cosine_similarity(feature1, feature2)\n",
    "\n",
    "def get_acc(y_score, y_true):\n",
    "    thresholds = sorted(set(y_score), reverse=True)\n",
    "    best_acc = 0\n",
    "    best_th = 0\n",
    "    for th in thresholds:\n",
    "        y_pred = (y_score >= th).astype(int)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_th = th\n",
    "    return best_acc, best_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay\n"
     ]
    }
   ],
   "source": [
    "# test the training step\n",
    "data_df = val_df.copy()    \n",
    "num_classes = len(np.unique(data_df['identity'].values))\n",
    "model = DataParallel(model_resnet)\n",
    "metric_fc = DataParallel(ArcMarginProduct(512, num_classes, s=30, m=0.5, easy_margin=False))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD([{'params': model.parameters()}, {'params': metric_fc.parameters()}], lr=0.001)\n",
    "model = model.train()\n",
    "\n",
    "for data, label in dataloader:\n",
    "    feature = model(data)\n",
    "    output = metric_fc(feature, label)\n",
    "    loss = criterion(output, label.long().cuda())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    output = output.data.cpu().numpy()\n",
    "    output = np.argmax(output, axis=1)\n",
    "    label = label.data.cpu().numpy()\n",
    "    acc = np.mean((output == label).astype(int))\n",
    "    break\n",
    "print('Okay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "data_df = data_df.copy()\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "validate_every = 1\n",
    "\n",
    "# dataloader\n",
    "train_dataset = FaceDataset(img_npy_list, label_npy_list, input_shape, 'train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# model, metric_fc, criterion, optimizer\n",
    "num_classes = len(np.unique(data_df['identity'].values))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "classes = data_df['identity_code'].unique()\n",
    "\n",
    "# model, metric_fc, criterion, optimizer\n",
    "model = DataParallel(model_resnet)\n",
    "metric_fc = DataParallel(ArcMarginProduct(512, num_classes, s=30, m=0.5, easy_margin=False))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD([{'params': model.parameters()}, {'params': metric_fc.parameters()}], lr=learning_rate)\n",
    "\n",
    "# writer\n",
    "exist_path = os.listdir('log')\n",
    "writer_version = int(exist_path[-1][-1]) + 1 if len(exist_path) > 0 else 0\n",
    "writer = SummaryWriter(log_dir=\"log/v\" + str(writer_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e75bf603a3046c68f3a67dc86e88abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88b5b5abf5d4d6e9448b13f82875dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for data, train_label in tqdm(dataloader, desc='Epoch {}/{}'.format(epoch+1, num_epochs)):\n",
    "        # forward\n",
    "        data = data.to(device)\n",
    "        train_label = train_label.to(device)\n",
    "        feature = model(data).to(device)\n",
    "        output = metric_fc(feature, train_label).to(device)\n",
    "        loss = criterion(output, train_label.long().cuda())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validate\n",
    "        if step % validate_every == 0:\n",
    "            model.eval()\n",
    "            similarity_list = []\n",
    "            label_list = []\n",
    "            with torch.no_grad():\n",
    "                for data1, data2, label in pair_dataloader:\n",
    "                    data1 = data1.to(device)\n",
    "                    data2 = data2.to(device)\n",
    "                    label = label.to(device)\n",
    "                    feature1, feature2 = get_feature(model, data1, data2)\n",
    "                    similarity = cosin_metric(feature1, feature2)\n",
    "                    similarity_list.append(similarity.cpu().numpy())\n",
    "                    label_list.append(label.cpu().numpy())\n",
    "                similarity_list = np.concatenate(similarity_list)\n",
    "                label_list = np.concatenate(label_list)\n",
    "                acc, th = get_acc(similarity_list, label_list)   \n",
    "\n",
    "            # visualize\n",
    "            img_grid = make_grid(data)\n",
    "            img = data.cpu().numpy()\n",
    "            img = img.reshape(img.shape[0], -1)\n",
    "            output = output.data.cpu().numpy()\n",
    "            output = np.argmax(output, axis=1)\n",
    "            train_label = train_label.data.cpu().numpy()\n",
    "\n",
    "            # writer.add_histogram('fc', model_resnet18.fc.weight, epoch)\n",
    "            writer.add_image('image', img_grid, step)\n",
    "            writer.add_scalar('loss', loss.item(), step)\n",
    "            writer.add_scalar('acc', acc, step)\n",
    "            writer.add_scalar('threshold', th, step)\n",
    "            writer.add_embedding(img, metadata=train_label, label_img=data, global_step=step)\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_path = 'model_save/model_resnet18.pth'\n",
    "metric_fc_path = 'model_save/metric_fc.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "torch.save(metric_fc.state_dict(), metric_fc_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
