{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter, DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import resnet50\n",
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. add batch size\n",
    "# 2. SDG\n",
    "# lr scheduler\n",
    "# weight decay (optimizer)\n",
    "# weight decay (lr scheduler)\n",
    "# add resnet50\n",
    "# change generator ****\n",
    "# validation\n",
    "\n",
    "# save to numpy\n",
    "# data.npy\n",
    "# label.npy\n",
    "# data_1.npy(shape) 500000,112,112,3\n",
    "# label_1.npy(shape) 500000,1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_df(identity_root, identity_list):\n",
    "    path_list = []\n",
    "    identity_label_list = []\n",
    "    for identity in tqdm(identity_list):\n",
    "        image_list = os.listdir(os.path.join(identity_root, identity)) \n",
    "        for img in image_list:\n",
    "            path = os.path.join(identity_root, identity, img)\n",
    "            path_list.append(path)\n",
    "            identity_label_list.append(identity) \n",
    "\n",
    "    data_df = pd.DataFrame({'img_path': path_list, 'identity': identity_label_list})\n",
    "    data_df['identity_code'] = pd.Categorical(data_df['identity']).codes # convert identity to unique code(int)\n",
    "    data_df['identity_code'] = data_df['identity_code'].astype('int32')\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5749/5749 [00:00<00:00, 7835.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# buid the dataframe from the the path\n",
    "identity_root = 'lfw_funneled'\n",
    "identity_list = os.listdir(identity_root)\n",
    "identity_list = [identity for identity in identity_list if os.path.isdir(os.path.join(identity_root, identity))] #only folder is identity\n",
    "\n",
    "# train test val split\n",
    "# train_identity_list, test_identity_list = train_test_split(identity_list, test_size=0.2, random_state=42)\n",
    "# test_identity_list, val_identity_list = train_test_split(test_identity_list, test_size=0.5, random_state=42)\n",
    "\n",
    "# build the dataframe\n",
    "# train_df = get_path_df(identity_root, train_identity_list)\n",
    "# test_df = get_path_df(identity_root, test_identity_list)\n",
    "# val_df = get_path_df(identity_root, val_identity_list)\n",
    "# print('train: {}, val: {}, test: {}'.format(len(train_df), len(val_df), len(test_df)))\n",
    "\n",
    "# total data for test\n",
    "path_df = get_path_df(identity_root, identity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array from path\n",
    "def get_img_npy(path_df, img_shape):\n",
    "    img_npy = np.zeros((len(path_df), img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
    "    label_npy = np.zeros((len(path_df), 1), dtype=np.uint8)\n",
    "    for i in tqdm(range(len(path_df))):\n",
    "        img_path = path_df.iloc[i]['img_path']\n",
    "        img = Image.open(img_path)\n",
    "        img_npy[i] = np.array(img)\n",
    "        label_npy[i] = path_df.iloc[i]['identity_code']\n",
    "    return img_npy, label_npy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13232/13232 [00:57<00:00, 229.44it/s]\n"
     ]
    }
   ],
   "source": [
    "df = path_df\n",
    "img_shape = (250, 250, 3)\n",
    "img_npy, label_npy = get_img_npy(df, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, img_npy_list, label_npy_list, input_shape, phase=\"train\"):\n",
    "        self.img_npy = np.vstack(img_npy_list)\n",
    "        self.label_npy = np.vstack(label_npy_list)\n",
    "        self.phase = phase\n",
    "        self.input_shape = input_shape\n",
    "        if self.phase == 'train':\n",
    "            self.transforms = T.Compose([\n",
    "                T.RandomCrop(self.input_shape[1:]),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "        else:   \n",
    "            self.transforms = T.Compose([\n",
    "                T.CenterCrop(self.input_shape[1:]),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.5], std=[0.5])\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.label_npy)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.img_npy[index]\n",
    "        label = self.label_npy[index]\n",
    "        data = Image.fromarray(data)\n",
    "        data = data.convert('L')\n",
    "        data = self.transforms(data)\n",
    "        return data.float(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "img_npy_list = [img_npy]\n",
    "label_npy_list = [label_npy]\n",
    "input_shape = (1, 250, 250)\n",
    "phase = 'train'\n",
    "dataset = FaceDataset(img_npy_list, label_npy_list, input_shape, phase)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "num_classes = len(np.unique(path_df['identity'].values))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 250, 250])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for data, label in dataloader:\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "model_resnet50 = resnet50(pretrained=False) \n",
    "# change first and last layer\n",
    "model_resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_resnet50.fc = nn.Linear(512, 512)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the training step\n",
    "data_df = val_df.copy()    \n",
    "num_classes = len(np.unique(data_df['identity'].values))\n",
    "model = DataParallel(model_resnet50)\n",
    "metric_fc = DataParallel(ArcMarginProduct(512, num_classes, s=30, m=0.5, easy_margin=False))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params': metric_fc.parameters()}], lr=0.001)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.37it/s]\n"
     ]
    }
   ],
   "source": [
    "for data, label in tqdm(dataloader):\n",
    "    feature = model(data)\n",
    "    output = metric_fc(feature, label)\n",
    "    loss = criterion(output, label.long().cuda())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    output = output.data.cpu().numpy()\n",
    "    output = np.argmax(output, axis=1)\n",
    "    label = label.data.cpu().numpy()\n",
    "    acc = np.mean((output == label).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "data_df = data_df.copy()\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_classes = len(np.unique(data_df['identity'].values))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "classes = data_df['identity_code'].unique()\n",
    "\n",
    "# model, metric_fc, criterion, optimizer\n",
    "model = DataParallel(model_resnet50)\n",
    "metric_fc = DataParallel(ArcMarginProduct(512, num_classes, s=30, m=0.5, easy_margin=False))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([{'params': model.parameters()}, {'params': metric_fc.parameters()}], lr=learning_rate)\n",
    "writer = SummaryWriter(log_dir='log/v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for data, label in tqdm(dataloader, desc='Epoch {}/{}'.format(epoch+1, num_epochs)):\n",
    "        # forward\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        feature = model(data).to(device)\n",
    "        output = metric_fc(feature, label).to(device)\n",
    "        loss = criterion(output, label.long().cuda())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # visualize\n",
    "        img_grid = make_grid(data)\n",
    "        img = data.cpu().numpy()\n",
    "        img = img.reshape(img.shape[0], -1)\n",
    "        output = output.data.cpu().numpy()\n",
    "        output = np.argmax(output, axis=1)\n",
    "        label = label.data.cpu().numpy()\n",
    "        acc = np.mean((output == label).astype(int))\n",
    "\n",
    "        # writer.add_histogram('fc', model_resnet18.fc.weight, epoch)\n",
    "        writer.add_image('image', img_grid, step)\n",
    "        writer.add_scalar('loss', loss.item(), step)\n",
    "        writer.add_scalar('acc', acc, step)\n",
    "        writer.add_embedding(img, metadata=label, label_img=data, global_step=step)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
